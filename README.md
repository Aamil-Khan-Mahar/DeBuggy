# DeBuggy: An Automated Debugging Pipeline

## Overview
DeBuggy is a modular and extensible project designed to automate the process of debugging code using cutting-edge LLMs (Large Language Models) and intuitive communication tools. It integrates five key modules into a seamless pipeline to receive, process, and resolve bug reports efficiently.

## Modules

### 1. MappingLLM
- **Purpose:** Creates mappings for code files, including file names and their descriptions.
- **Output:** A structured JSON object detailing the file name, description, and other relevant metadata.

### 2. FinderLLM
- **Purpose:** Matches bug reports with the appropriate code file using mappings created by `MappingLLM`.
- **Output:** The name of the most relevant code file.

### 3. DebuggingLLM
- **Purpose:** Retrieves the code from the identified file and performs debugging operations.
- **Output:** Suggestions or fixed code based on identified issues.

### 4. EmailListener
- **Purpose:** Receives bug reports via email and processes them into a structured format.
- **Output:** Parsed bug reports ready for pipeline input.

### 5. SlackListener
- **Purpose:** Integrates Slack messaging for receiving and processing bug reports.
- **Output:** Bug reports received as Slack messages, formatted for further processing.

## Pipeline
The `Pipeline.py` file combines all the above modules into a cohesive system that:
1. Listens for bug reports via email and Slack.
2. Maps the report to the relevant code files.
3. Identifies and resolves issues using LLMs.
4. Outputs a structured response for developers.

## MappingLLM: Code File Mapping Model

### Overview
The `MappingLLM` module is responsible for analyzing and mapping code files to descriptive metadata. It uses OpenAI's GPT-4 model to process code and generate a structured JSON object containing function names, descriptions, variables, and class details.

### Key Features
- **Input:** Raw code from a file.
- **Output:** JSON object containing:
  - File name
  - Description
  - Variables with descriptions, types, values, and scopes
  - Functions with descriptions, parameters, return values, and scopes
  - Classes with descriptions and methods
- **Model Used:** OpenAI GPT-4.

### Methods

#### `__init__(choice='GPT-4')`
Initializes the `MappingLLM` instance.
- **Parameters:**
  - `choice` (default `'GPT-4'`): Specifies the model to be used.

#### `infer(code)`
Generates the JSON mapping for the provided code.
- **Parameters:**
  - `code`: The source code to be analyzed.
- **Returns:**
  - A JSON object containing the mapping information.

#### `get_last_response()`
Retrieves the last model response.
- **Returns:**
  - The previously generated JSON mapping.

### Example Workflow
```python
from MappingLLM import MappingLLM

# Initialize the model
mapping_llm = MappingLLM(choice='GPT-4')

# Example code to analyze
code = '''
class Math:
    def add(self, a, b):
        return a + b
'''

# Generate a mapping
mapping = mapping_llm.infer(code)
print(mapping)
```
## FinderLLM: Code File Finder Model

### Overview
The `FinderLLM` module identifies the most relevant code file based on a user-provided bug report and a set of code file descriptions generated by the `MappingLLM`. Using OpenAI's GPT-4 model, it outputs the filename, reasoning, and confidence level in a structured JSON format.

### Key Features
- **Input:**
  - Bug report (text describing the issue).
  - Code mappings (JSON object with file descriptions from `MappingLLM`).
- **Output:** JSON object containing:
  - Filename
  - Reasoning
  - Confidence level
- **Model Used:** OpenAI GPT-4.

### Methods

#### `__init__(choice='GPT-4')`
Initializes the `FinderLLM` instance.
- **Parameters:**
  - `choice` (default `'GPT-4'`): Specifies the model to be used.

#### `set_code_mappings(code_mappings)`
Sets the code mappings used for finding the relevant file.
- **Parameters:**
  - `code_mappings`: A JSON object containing file descriptions.

#### `find_code_filename(bug_report)`
Finds the most relevant code file based on the bug report and code mappings.
- **Parameters:**
  - `bug_report`: Text describing the issue.
- **Returns:**
  - A JSON object containing the filename, reasoning, and confidence level.

#### `get_last_response()`
Retrieves the last model response.
- **Returns:**
  - The previously generated JSON object with the findings.

### Example Workflow
```python
from FinderLLM import FinderLLM

# Initialize the model
finder_llm = FinderLLM(choice='GPT-4')

# Set code mappings
code_mappings = {
    "example.py": "This file contains basic math operations.",
    "string_utils.py": "This file handles string manipulation functions."
}
finder_llm.set_code_mappings(code_mappings)

# Example bug report
bug_report = "The addition function is not returning the correct result when given large numbers."

# Find the most relevant code file
finding = finder_llm.find_code_filename(bug_report)
print(finding)
```

## DebuggerLLM: Code Debugging Model

### Overview
The `DebuggerLLM` module is designed to identify and fix bugs in code files using bug reports and source code provided by the `FinderLLM` module. It uses OpenAI's GPT-4 to generate corrected code files with comments indicating the changes made.

### Key Features
- **Input:**
  - Bug report (text describing the issue).
  - Code file content (string containing the code).
- **Output:** JSON object containing:
  - Corrected code file with detailed comments above the fixed lines.
- **Model Used:** OpenAI GPT-4.

### Methods

#### `__init__(choice='GPT-4')`
Initializes the `DebuggerLLM` instance.
- **Parameters:**
  - `choice` (default `'GPT-4'`): Specifies the model to be used.

#### `debug(bug_report, code_file)`
Processes the bug report and corresponding code file to fix the identified bugs.
- **Parameters:**
  - `bug_report`: Text describing the issue.
  - `code_file`: The code file content as a string.
- **Returns:**
  - A JSON object with the corrected code file.

#### `get_last_response()`
Retrieves the last model response.
- **Returns:**
  - The previously generated JSON object containing the corrected code.

### Example Workflow
```python
from DebuggerLLM import DebuggerLLM
from FinderLLM import FinderLLM

# Initialize models
debugger_llm = DebuggerLLM(choice='GPT-4')
finder_llm = FinderLLM(choice='GPT-4')

# Load code mappings
with open('../Data/Json/mappings.json', 'r') as file:
    code_mappings = json.load(file)

finder_llm.set_code_mappings(code_mappings)

# Example bug report
bug_report = "The add function is not returning correct results."

# Find the relevant code file
filename = finder_llm.find_code_filename(bug_report)["Filename"]

# Load buggy code file
with open(f'../Data/Buggy/{filename}', 'r') as file:
    buggy_code = file.read()

# Debug the code
debugged_code_response = debugger_llm.debug(bug_report, buggy_code)
print(debugged_code_response)
```

## EmailListener: Email Reception Tool

### Overview
The `EmailListener` module is responsible for monitoring a specified email account for incoming bug reports. The module connects to the email inbox, checks for new emails, and processes them into a structured format. It uses IMAP protocol to retrieve emails and handles reports based on their subjects and content.

### Key Features
- **Input:**
  - Email account credentials (email address and app password).
  - Folder to monitor (default is 'INBOX').
  - Time interval between checks for new emails (set to 3 seconds).
- **Output:**
  - Reports are stored with details such as the sender, subject, content, and a unique report ID.
  
### Methods

#### `__init__(email, password, folder='INBOX')`
Initializes the `EmailListener` instance.
- **Parameters:**
  - `email`: The email address used to connect to the mailbox.
  - `password`: The app-specific password for the email account (especially when 2FA is enabled).
  - `folder`: The folder to monitor for emails (default is 'INBOX').

#### `connect()`
Establishes a connection to the email inbox and continuously checks for new emails.
- **Runs indefinitely**, checking every 3 seconds for new emails and processing them.

#### `check_emails()`
Checks for new, unseen emails in the specified folder and processes them.
- **Processes emails** by storing their sender, subject, and content. Each new report is assigned a unique report ID.
- **Saves processed reports** in the `reports` list and tracks newly fetched reports separately in `new_reports`.

#### `get_reports()`
Returns all the reports collected so far.
- **Returns:**
  - A list of all reports, each containing the sender, subject, text, and report ID.

#### `get_new_reports()`
Returns only the newly received reports since the last check.
- **Returns:**
  - A list of new reports and clears the `new_reports` list after fetching them.

### Example Usage
```python
from EmailListener import EmailListener

# Initialize the listener and connect to the inbox
listener = EmailListener() 
listener.connect()

# Fetch all reports (can be done after a while to ensure reports are collected)
all_reports = listener.get_reports()

# Fetch only new reports
new_reports = listener.get_new_reports()
```

## SlackListener: Slack Message Reception Tool

### Overview
The `SlackListener` module is responsible for receiving and processing messages from a specific Slack channel. It listens for incoming messages and stores them as reports. This tool is built using Flask and the Slack Events API, enabling real-time interaction with Slack events.

### Key Features
- **Input:**
  - Slack credentials stored in a `.env` file (`SLACK_TOKEN` and `SLACK_SIGNING_SECRET`).
  - The port for the Flask app to run (default is `8000`).
  - A designated Slack channel (`report_channel_id`) to monitor for reports.
- **Output:**
  - Messages from the monitored Slack channel are collected and stored with user ID, channel ID, and message text.

### Methods

#### `__init__(env_path='.env', port=8000)`
Initializes the `SlackListener` instance.
- **Parameters:**
  - `env_path`: The path to the `.env` file containing Slack credentials (`SLACK_TOKEN` and `SLACK_SIGNING_SECRET`).
  - `port`: The port on which the Flask app will run (default is 8000).

#### `handle_message(payload)`
Handles incoming message events from Slack.
- **Parameters:**
  - `payload`: The event payload received from Slack.
- **Actions:**
  - It filters messages from the specified channel (`report_channel_id`) and excludes those sent by the bot itself.
  - The received messages are stored in the `reports` list, along with the user ID, channel ID, and message text.

#### `get_messages()`
Retrieves all messages received so far.
- **Returns:**
  - A list of messages, each containing the user ID, channel ID, and message text.

#### `start()`
Starts the Slack listener.
- **Actions:**
  - Begins running the Flask app and listening for Slack events, enabling real-time processing of messages in the specified Slack channel.

### Example Usage
```python
from SlackListener import SlackListener

# Initialize the listener with the .env file and port
listener = SlackListener(env_path=".env", port=8000)

# Start the listener
listener.start()

# Retrieve all received messages
all_messages = listener.get_messages()
```